# VIALM
This repository offers a survey and benchmark for Visual Impairment Assistance using Language Models (VIALM).
## 1. Task Illustration
AThe following image is a sample input and output of VIALM.
Its input is a pair of a visual image of the environment (the left image) and a user request in human language (the grey box).
The yellow box shows the output guidance for VI users to complete the request within the environment (the right image).
The output should grounded and fine-grained for VI users to follow easily.
![VIALM](./images/VIALM_task.png 'VIALM_task')
## 2. Paper Collections
For the survey part, the following are related papers. It will be continually updated. The following image is a summary of timeline.
![Timeline](./images/timeline.png 'LM Timeline')
### 2.1 Large Language models (LLMs)
* Teven Le Scao, Angela Fan, and et al. BLOOM: A 176b-parameter open-access multilingual language model. arXiv, 2211.05100, 2022.
* Nan Du, Yanping Huang, and et al. GLaM: Efficient scaling of language models with mixture-of-experts. In ICML,2022.
### 2.2 Visual-Language Models (VLMs)
* Danny Driess, Fei Xia, and et al. PaLM-E: An embodied multimodal language model. In ICML, 2023.

* Wenbo Hu, Yifan Xu, and et al. BLIVA: A simple multimodal LLM for better handling of text-rich visual questions. arXiv, 2308.09936, 2023.

* Srinivasan Iyer, Xi Victoria Lin, and et al. OPT-IML: scaling language model instruction meta learning through the lens of generalization. arXiv, 2212.12017, 2022.

* Junnan Li, Dongxu Li, and et al. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023.

* Jacky Liang, Wenlong Huang, and et al. Code as Policies:Language model programs for embodied control. In ICRA, 2023.

* Haotian Liu, Chunyuan Li, and et al. Improved baselines with visual instruction tuning. arXiv, 2310.03744, 2023.
* Haotian Liu, Chunyuan Li, and et al. Visual instruction tuning. arXiv, 2304.08485, 2023.
* James Manyika. An overview of bard: an early experiment with generative ai. AI. Google Static Documents, 2023.


* Bo Peng, Eric Alcaide, and et al. RWKV: Reinventing rnns for the transformer era. In EMNLP (Findings), 2023.



### 2.3 Embodied Agents
* Wenlong Huang, Fei Xia, and et al. Inner monologue: Embodied reasoning through planning with language models. In CoRL, 2022.
* Wenlong Huang, Fei Xia, and et al. Grounded decoding: Guiding text generation with grounded models for robot control. arXiv, 2303.00855, 2023.
* Brian Ichter, nthony Brohan, and et al. Do as I can, not as I say: Grounding language in robotic affordances. In CoRL, 2022.
* Ishika Singh, Valts Blukis, and et al. ProgPrompt: Generating situated robot task plans using large language models. In ICRA, 2023.
* Chan Hee Song, Jiaman Wu, and et al. Llm-planner: Few-shot grounded planning for embodied agents with large language models. arXiv, 2212.04088, 2022.
## 3. Benchmark Evaluation
### 3.1 Benchmark Annotations
### 3.2 LM Predictions



